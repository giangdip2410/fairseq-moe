bash-4.2$ ssh epyc07 bash[C[C[C[C[C[C[C[C[C[C[Kmore
Usage: more [options] file...

Options:
  -d        display help instead of ring bell
  -f        count logical, rather than screen lines
  -l        suppress pause after form feed
  -p        do not scroll, clean screen and display text
  -c        do not scroll, display text and clean line ends
  -u        suppress underlining
  -s        squeeze multiple blank lines into one
  -NUM      specify the number of lines per screenful
  +NUM      display file beginning from line number NUM
  +/STRING  display file beginning from search string match
  -V        output version information and exit
bash-4.2$ moressh epyc07 bash[C[C[C[C[C[C[C[C[C[C[C[9Pqueuesh epyc13 bash[C[C[C[C[C[C[C[C[C[C[C[9Pqueue[1Pinfoqueuesh epyc13 bash[C[C[C[C[C[C[C[C[C[C[C[9Pqueue[1Pinfoqueue[1Pinfoqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/[C[C[C[C[C[C[C[C[C[Cls[Kcd CompeteMOE/[C[C[C[C[C[C[C[C[C[Cls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[Cls[Kcd train_scripts/[C[C[C[C[C[C[C[C[C[Cls[Kpwd[1Plscd fairseq-moe/[C[C[C[C[C[C[C[C[C[Cls[Kcd Fair_MOE/[C[C[C[C[C[C[C[C[C[Cls[Kcd ICML/ls[Kconda activate fairmoe[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[Chistory 1000[K[C[C[C[C[C[C[C[C[C[C[2Pnvidia-smi[5Psinfoqueuecancel 7348[6Pqueuecancel 7348[6Pqueuenvidia-smi[3Phistory[K[K[K[K[K[K[Klsnvidia-smi[4Psqueuecancel 7348[6Pqueuecancel 7348[6Pqueue[1Pinfonvidia-smihistory 1000[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[Cls[Kcd ICML/ls[Kcd Fair_MOE/[C[C[C[C[C[C[C[C[C[Cls[Kcd fairseq-moe/[C[C[C[C[C[C[C[C[C[Cls[Kpwd[1Plscd train_scripts/[C[C[C[C[C[C[C[C[C[Cls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[Cls[Kcd CompeteMOE/[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/[C[C[C[C[C[C[C[C[C[Cls[Kbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[Cexit[Ksqueue[1Pinfoqueue[1Pinfoqueuesh epyc13 bash[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Knvidia-smi
Fri Nov 10 02:21:08 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100 80G...  Off  | 00000000:01:00.0 Off |                    0 |
| N/A   34C    P0    42W / 300W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100 80G...  Off  | 00000000:A1:00.0 Off |                    0 |
| N/A   33C    P0    44W / 300W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
bash-4.2$ conda activate fairmoe

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


bash-4.2$ conda activate fairmoe[C[C[C[C[C[C[C[C[C[C[12Pnvidia-smi[6Pmoressh epyc07 bash[C[C[C[C[C[C[C[C[C[C[C[9Pqueuesh epyc13 bash[C[C[C[C[C[C[C[C[C[C[C[9Pqueue[1Pinfoqueuesh epyc13 bash[C[C[C[C[C[C[C[C[C[C[C[9Pqueue[1Pinfoqueue[1Pinfoqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/[C[C[C[C[C[C[C[C[C[Cls[Kcd CompeteMOE/[C[C[C[C[C[C[C[C[C[Cls[K[K[Ksource /opt/share/modules/anaconda3/etc/profile.d/conda.sh
bash-4.2$ conda activate fairmoe
(fairmoe) bash-4.2$ ls
filename			    xmoe_brainformer_medium_wt2_eval.sh
screenlog.0			    xmoe_brainformer_medium_wt2.sh
xmoe_brainformer_large_wt2_eval.sh  xmoe_brainformer_small_wt2_eval.sh
xmoe_brainformer_large_wt2.sh	    xmoe_brainformer_small_wt2.sh
(fairmoe) bash-4.2$ owd[K[K[Kpwd
/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/train_scripts/XMOE/Wikitext-2
(fairmoe) bash-4.2$ xmoe_brainformer_small_wt2.sh
bash: xmoe_brainformer_small_wt2.sh: command not found
(fairmoe) bash-4.2$ xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cbxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Caxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
../../checkpoints/xmoe_brainformer_lm_wt2_small directory exists.
2023-11-10 02:22:52 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16650
2023-11-10 02:22:52 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16650
2023-11-10 02:22:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-11-10 02:22:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-11-10 02:22:52 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-11-10 02:22:52 | INFO | fairseq.distributed.utils | initialized host epyc13 as rank 0
2023-11-10 02:22:52 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-11-10 02:22:52 | INFO | fairseq.distributed.utils | initialized host epyc13 as rank 1
dummy test all-reduce success!
2023-11-10 02:22:54 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0283203125, 'gpu_1_mem_used_gb': 1.0263671875}
2023-11-10 02:22:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'tqdm', 'log_file': '../../checkpoints/xmoe_brainformer_lm_wt2_small/log.txt', 'tensorboard_logdir': '../tblogs/xmoe_brainformer_lm_wt2_small', 'wandb_project': None, 'azureml_logging': False, 'seed': 2410, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'xmoe', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16650', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'distributed_ifname_list': '', 'distributed_gpu_list': '', 'distributed_num_procs': 2}, 'dataset': {'_name': None, 'num_workers': 2, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0007], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../checkpoints/xmoe_brainformer_lm_wt2_small', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '-rank-0', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'assignment_type': 'none'}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'brainformerlm_tiny', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 256, 'decoder_output_dim': 256, 'decoder_input_dim': 256, 'decoder_ffn_embed_dim': 256, 'decoder_layers': 2, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'moe_freq': 1, 'moe_expert_count': 16, 'moe_gating_use_fp32': True, 'moe_second_expert_policy': 'random', 'moe_normalize_gate_prob_before_dropping': True, 'moe_expert_ffn_dim': None, 'moe_top1_expert': False, 'moe_eval_capacity_token_fraction': -1.0, 'moe_normalize_expert_grad': 'world_size', 'record_a2a_perf_stats': False, 'dummy_a2a': False, 'moe_batch_prioritized_routing': False, 'use_xmoe': True, 'add_bos_token': False, 'tokens_per_sample': 128, 'max_target_positions': None, 'tpu': False, 'memory_efficient_fp16': True, 'fp16': True, 'fp16_no_flatten_grads': False, 'ddp_backend': 'no_c10d', 'world_size': 2, 'distributed_rank': 0, 'ddp_rank': 0, 'deepnorm': False, 'subln': False, 'rel_pos_buckets': 0, 'max_rel_pos': 0, 'xpos_rel_pos': False, 'xpos_scale_base': 512}, 'task': {'_name': 'language_modeling', 'data': '/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/preprocess_scripts/data-bin/wikitext-2', 'sample_break_mode': 'none', 'tokens_per_sample': 128, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_source_positions': None, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 2410, 'batch_size': 128, 'batch_size_valid': 128, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'moe_cross_entropy', 'moe_gate_loss_wt': 0.01, 'moe_gate_loss_combine_method': 'sum', 'moe_gate_loss_transform': 'none', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0007], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 1000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 5000.0, 'lr': [0.0007]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2023-11-10 02:22:54 | INFO | fairseq.tasks.language_modeling | dictionary: 33280 types
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:3 to store for rank: 0
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 2 nodes.
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:4 to store for rank: 0
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 2 nodes.
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:5 to store for rank: 0
2023-11-10 02:22:54 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 2 nodes.
2023-11-10 02:22:54 | INFO | fairseq_cli.train | LanguageModel(
  (decoder): LMDecoder(
    (dropout_module): Dropout(p=0.1, inplace=False)
    (embed_tokens): Embedding(33280, 256, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (output_projection): Linear(in_features=256, out_features=33280, bias=False)
    (layers): ModuleList(
      (0-1): 2 x BrainformerDecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn1): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn2): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
        )
        (ffn3): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
        )
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=256, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0-7): 8 x FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (fc2): Linear(in_features=256, out_features=256, bias=True)
            )
          )
        )
        (moe_layer1): MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=256, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0-7): 8 x FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (fc2): Linear(in_features=256, out_features=256, bias=True)
            )
          )
        )
        (moe_layer2): MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=256, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0-7): 8 x FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (fc2): Linear(in_features=256, out_features=256, bias=True)
            )
          )
        )
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)
2023-11-10 02:22:54 | INFO | fairseq_cli.train | task: LanguageModelingTask
2023-11-10 02:22:54 | INFO | fairseq_cli.train | model: LanguageModel
2023-11-10 02:22:54 | INFO | fairseq_cli.train | criterion: MoECrossEntropyCriterion
2023-11-10 02:22:54 | INFO | fairseq_cli.train | num. non-expert model params: 10,133,504 (num. trained: 10,133,504)
2023-11-10 02:22:54 | INFO | fairseq_cli.train | num. expert model params: 6,316,032 (num. trained: 6,316,032)
2023-11-10 02:22:54 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0283203125, 'gpu_1_mem_used_gb': 1.0283203125}
2023-11-10 02:22:54 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: /home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/preprocess_scripts/data-bin/wikitext-2/valid
2023-11-10 02:22:55 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-11-10 02:22:55 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.0.moe_layer.gate.wg_reduction.bias
2023-11-10 02:22:55 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.0.moe_layer1.gate.wg_reduction.bias
2023-11-10 02:22:55 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.0.moe_layer2.gate.wg_reduction.bias
2023-11-10 02:22:55 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer.gate.wg_reduction.bias
2023-11-10 02:22:55 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer1.gate.wg_reduction.bias
2023-11-10 02:22:55 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer2.gate.wg_reduction.bias
2023-11-10 02:22:55 | INFO | fairseq.trainer | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0283203125, 'gpu_1_mem_used_gb': 1.0283203125}
2023-11-10 02:22:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2023-11-10 02:22:55 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.185 GB ; name = NVIDIA A100 80GB PCIe                   
2023-11-10 02:22:55 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 79.185 GB ; name = NVIDIA A100 80GB PCIe                   
2023-11-10 02:22:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2023-11-10 02:22:55 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2023-11-10 02:22:55 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 128
2023-11-10 02:22:55 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0830078125, 'gpu_1_mem_used_gb': 1.0830078125}
2023-11-10 02:22:55 | INFO | fairseq.trainer | Preparing to load checkpoint ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-rank-0.pt
2023-11-10 02:22:55 | INFO | fairseq.trainer | Loaded state for ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-rank-0.pt
2023-11-10 02:22:55 | INFO | fairseq.trainer | prepare to feed parameters to model
2023-11-10 02:22:55 | INFO | fairseq.trainer | finish loading parameters for model
2023-11-10 02:23:02 | INFO | fairseq.trainer | Loaded optim_state for ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-rank-0.pt
2023-11-10 02:23:02 | INFO | fairseq.trainer | Loaded checkpoint ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-rank-0.pt (epoch 1252 @ 5000 updates)
2023-11-10 02:23:02 | INFO | fairseq.trainer | loading train data for epoch 1252
2023-11-10 02:23:02 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: /home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/preprocess_scripts/data-bin/wikitext-2/train
epoch 1252:   0%|                                                     | 0/4 [00:00<?, ?it/s]2023-11-10 02:23:02 | INFO | fairseq.trainer | begin training epoch 1252
2023-11-10 02:23:02 | INFO | fairseq_cli.train | Start iterating over samples
/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/fairseq/../fairseq/utils.py:360: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/fairseq/../fairseq/utils.py:360: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2023-11-10 02:23:12 | INFO | fairseq_cli.train | Stopping training due to num_updates: 5001 >= max_update: 5000
2023-11-10 02:23:12 | INFO | fairseq_cli.train | begin validation on "valid" subset on rank 0
2023-11-10 02:23:12 | INFO | fairseq_cli.train | got valid iterator on "valid" subset on rank 0

epoch 1252 | valid on 'valid' subset:   0%|                           | 0/7 [00:00<?, ?it/s][A2023-11-10 02:23:12 | INFO | fairseq_cli.train | Begin looping over validation "valid" subset with length "7"

epoch 1252 | valid on 'valid' subset:  14%|██▋                | 1/7 [00:00<00:01,  4.74it/s][A
epoch 1252 | valid on 'valid' subset:  29%|█████▍             | 2/7 [00:00<00:00,  5.17it/s][A
epoch 1252 | valid on 'valid' subset:  43%|████████▏          | 3/7 [00:00<00:00,  5.32it/s][A
epoch 1252 | valid on 'valid' subset:  57%|██████████▊        | 4/7 [00:00<00:00,  5.40it/s][A
epoch 1252 | valid on 'valid' subset:  71%|█████████████▌     | 5/7 [00:00<00:00,  5.43it/s][A
epoch 1252 | valid on 'valid' subset:  86%|████████████████▎  | 6/7 [00:01<00:00,  5.46it/s][A
epoch 1252 | valid on 'valid' subset: 100%|███████████████████| 7/7 [00:01<00:00,  5.46it/s][A
                                                                                            [A2023-11-10 02:23:14 | INFO | valid | epoch 1252 | valid on 'valid' subset | loss 11.291 | moe_gate_loss 6.07483 | overflow_expert1 0 | overflow_expert2 0 | entropy_gating 0.302 | expert1_balance_top 34.004 | expert1_balance_bottom 19.547 | unused_expert1_count 0 | expert2_balance_top 38.903 | expert2_balance_bottom 15.463 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | unrouted_token_rate 0 | sparsity 0 | inner_loss 11.203 | ppl 2357.44 | wps 168600 | wpb 31092.3 | bsz 243 | num_updates 5001 | best_loss 7.3
2023-11-10 02:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1252 @ 5001 updates
2023-11-10 02:23:14 | INFO | fairseq.trainer | Saving checkpoint to ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-rank-0.pt
2023-11-10 02:23:14 | INFO | fairseq.trainer | Finished saving checkpoint to ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-rank-0.pt
2023-11-10 02:23:14 | INFO | fairseq.trainer | Saving checkpoint to ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-shared.pt
2023-11-10 02:23:14 | INFO | fairseq.trainer | Finished saving checkpoint to ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-shared.pt
2023-11-10 02:23:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../checkpoints/xmoe_brainformer_lm_wt2_small/checkpoint_last-rank-0.pt (epoch 1252 @ 5001 updates, score 11.291) (writing took 0.4523485880345106 seconds)
                                                                                            2023-11-10 02:23:14 | INFO | fairseq_cli.train | end of epoch 1252 (average epoch stats below)
2023-11-10 02:23:14 | INFO | train | epoch 1252 | loss 1.458 | moe_gate_loss 6.07321 | overflow_expert1 0 | overflow_expert2 0 | entropy_gating 0.217 | expert1_balance_top 28.328 | expert1_balance_bottom 21.513 | unused_expert1_count 0 | expert2_balance_top 32.873 | expert2_balance_bottom 17.603 | unused_expert2_count 0 | all_to_all_cpu_time_ms 0 | all_to_all_cuda_time_ms 0 | unrouted_token_rate 0 | sparsity 0 | inner_loss 1.37 | ppl 2.58 | wps 43711.7 | ups 0.08 | wpb 524250 | bsz 4096 | num_updates 5001 | lr 0 | gnorm 0.183 | loss_scale 16 | train_wall 10 | cuda_gb_allocated 21.2 | cuda_gb_reserved 25.9 | cuda_gb_free 58 | wall 0
2023-11-10 02:23:14 | INFO | fairseq_cli.train | done training in 12.7 seconds
(fairmoe) bash-4.2$ /home/gtruong/.conda/envs/fairmoe/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
bash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[5Pxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpwd[K[1Plsconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12Pnvidia-smi[6Pmoressh epyc07 bash[9Pqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueue[1Pinfoqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/ls[Kcd CompeteMOE/ls[Kcd CompeteMOE/ls[Kcd Wikitext-2/ls[Kbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cexit[Ksqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/ls[Kcd CompeteMOE/ls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd train_scripts/ls[Kpwd[1Plscd fairseq-moe/ls[Kcd fairseq-moe/ls[Kpwd[1Plscd train_scripts/ls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd CompeteMOE/ls[Kcd Wikitext-2/ls[Kbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cexit[Ksqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/ls[Kcd CompeteMOE/ls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd train_scripts/ls[Kpwd[1Plscd fairseq-moe/ls[Kcd Fair_MOE/ls[Kcd ICML/ls[Kconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chistory 1000[K[2Pnvidia-smi[5Psinfoqueuecancel 7348[6Pqueuecancel 7348[6Pqueuenvidia-smils[Ksrun --time=115:00:00 --nodelist=epyc08 --partition=gpu --ntasks=8 --gress=gpu:1 --pty /bin/bash -lMinfo[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cqueue[1Pinfoqueue[1Pinfo^C
(fairmoe) bash-4.2$ bash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[5Pxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpwd[K[1Plsconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12Pnvidia-smi[6Pmoressh epyc07 bash[9Pqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueue[1Pinfoqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/ls[Kcd CompeteMOE/ls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd train_scripts/ls[Kpwd[1Plscd fairseq-moe/ls[Kcd Fair_MOE/ls[Kcd fairseq-moe/ls[Kpwd[1Plscd train_scripts/ls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd CompeteMOE/ls[Kcd Wikitext-2/ls[Kbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cexit[Ksqueue[1Pinfoqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueuesh epyc07 bashmore[Knvidia-smiconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kpwdxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cbash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K[K(fairmoe) bash-4.2$ [K(fairmoe) bash-4.2$ bash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[5Pxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpwd[K[1Plsconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12Pnvidia-smi[6Pmoressh epyc07 bash[9Pqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueue[1Pinfoqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/ls[Kcd CompeteMOE/ls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd train_scripts/ls[Kpwd[1Plscd fairseq-moe/ls[Kcd Fair_MOE/ls[Kcd ICML/ls[Kconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chistory 1000[K[2Pnvidia-smi[5Psinfoqueuecancel 7348[6Pqueuecancel 7348[6Pqueuenvidia-smi[4Psqueuecancel 7348[6Pqueuecancel 7348[6Pqueue[1Pinfonvidia-smihistory 1000source /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd ICML/ls[Kcd Fair_MOE/ls[K^C
(fairmoe) bash-4.2$ bash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[5Pxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpwd[K[1Plsconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12Pnvidia-smi[6Pmoressh epyc07 bash[9Pqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueue[1Pinfoqueue[1Pinfoqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueuesh epyc07 bashmore[Knvidia-smiconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kpwdxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cbash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kbash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[5Pxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpwd[K[1Plsconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12Pnvidia-smi[6Pmoressh epyc07 bash[9Pqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueue[1Pinfoqueue[1Pinfoqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueuesh epyc07 bashmore[Knvidia-smiconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kpwdxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cbash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kbash xmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[5Pxmoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpwd[K[1Plsconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[36Pconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12Pnvidia-smi[6Pmoressh epyc07 bash[9Pqueuesh epyc13 bash[9Pqueue[1Pinfoqueuesh epyc13 bash[9Pqueue[1Pinfoqueue[1Pinfoqueue[2Pexitbash competemoe_brainformer_small_wt2.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/ls[Kcd CompeteMOE/ls[Kcp -r XMOE/ CompeteMOE[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd train_scripts/ls[Kpwd[1Plscd fairseq-moe/ls[Kcd Fair_MOE/ls[Kcd ICML/ls[Kconda activate fairmoe[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Csource /opt/share/modules/anaconda3/etc/profile.d/conda.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chistory 1000[K[2Pnvidia-smi[5Psinfoqueuecancel 7348[6Pqueuecancel 7348[6Pqueuenvidia-smils[Ksrun --time=115:00:00 --nodelist=epyc08 --partition=gpu --ntasks=8 --gress=gpu:1 --pty /bin/bash -lMinfo[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cqueue[1Pinfoqueue[1Pinfofnoqueue[4Plssqueuecancel 7791[6Pqueue[4Plsscancel 7786[8Pexit[2Plscp -r xmoe competemoepwd[K[1Plscd ..[3Plscd ..[3Plscp -r Wikitext-2/ Enwik8[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[3Pmv Enwik8/ Enwik8_oldls[Kcd XMOE/ls[Kbash brainformer_xmoe.sh [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[15Pnvidia-smils[Kcp -r SMOE/Wikitext-2/ XMOE/[5PEnwik8[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd ..[3Plscd ..[3Plsbash smoe_brainformer_small_wt2_test_eval.sh[5P.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcp -r smoe_brainformer_small_wt2.sh smoe_brainformer_small_wt2_test.sh[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd Wikitext-2/ls[Kcd SMOE/[K[2Plscd train_scripts/ls[Kbash train_scripts/[K(fairmoe) bash-4.2$ bash train_scripts/[K(fairmoe) bash-4.2$ bash train_scripts/[K(fairmoe) bash-4.2$ bash train_scripts/[K(fairmoe) bash-4.2$ bash train_scripts/^C
(fairmoe) bash-4.2$ ^C
(fairmoe) bash-4.2$ ^C
(fairmoe) bash-4.2$ ^C
(fairmoe) bash-4.2$ clear
[H[J(fairmoe) bash-4.2$ pwd
/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/train_scripts/XMOE/Wikitext-2
(fairmoe) bash-4.2$ ls
filename			    xmoe_brainformer_medium_wt2_eval.sh
screenlog.0			    xmoe_brainformer_medium_wt2.sh
xmoe_brainformer_large_wt2_eval.sh  xmoe_brainformer_small_wt2_eval.sh
xmoe_brainformer_large_wt2.sh	    xmoe_brainformer_small_wt2.sh
(fairmoe) bash-4.2$ bash xmoe_brainformer_medium_wt2.sh
2023-11-10 02:33:35 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14498
2023-11-10 02:33:35 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14498
2023-11-10 02:33:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-11-10 02:33:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-11-10 02:33:36 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-11-10 02:33:36 | INFO | fairseq.distributed.utils | initialized host epyc13 as rank 0
2023-11-10 02:33:36 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2023-11-10 02:33:36 | INFO | fairseq.distributed.utils | initialized host epyc13 as rank 1
dummy test all-reduce success!
2023-11-10 02:33:37 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0283203125, 'gpu_1_mem_used_gb': 1.0283203125}
2023-11-10 02:33:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'tqdm', 'log_file': '/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/train_scripts/XMOE/checkpoints/xmoe_brainformer_lm_wt2_medium/log.txt', 'tensorboard_logdir': '../tblogs/xmoe_brainformer_lm_wt2_medium', 'wandb_project': None, 'azureml_logging': False, 'seed': 2410, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'xmoe', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14498', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'distributed_ifname_list': '', 'distributed_gpu_list': '', 'distributed_num_procs': 2}, 'dataset': {'_name': None, 'num_workers': 8, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0007], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../checkpoints/xmoe_brainformer_lm_wt2_medium', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '-rank-0', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'assignment_type': 'none'}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'brainformerlm_small', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 256, 'decoder_output_dim': 256, 'decoder_input_dim': 256, 'decoder_ffn_embed_dim': 512, 'decoder_layers': 4, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'moe_freq': 1, 'moe_expert_count': 16, 'moe_gating_use_fp32': True, 'moe_second_expert_policy': 'random', 'moe_normalize_gate_prob_before_dropping': True, 'moe_expert_ffn_dim': None, 'moe_top1_expert': False, 'moe_eval_capacity_token_fraction': -1.0, 'moe_normalize_expert_grad': 'world_size', 'record_a2a_perf_stats': False, 'dummy_a2a': False, 'moe_batch_prioritized_routing': False, 'use_xmoe': True, 'add_bos_token': False, 'tokens_per_sample': 128, 'max_target_positions': None, 'tpu': False, 'memory_efficient_fp16': True, 'fp16': True, 'fp16_no_flatten_grads': False, 'ddp_backend': 'no_c10d', 'world_size': 2, 'distributed_rank': 0, 'ddp_rank': 0, 'deepnorm': False, 'subln': False, 'rel_pos_buckets': 0, 'max_rel_pos': 0, 'xpos_rel_pos': False, 'xpos_scale_base': 512}, 'task': {'_name': 'language_modeling', 'data': '/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/preprocess_scripts/data-bin/wikitext-2', 'sample_break_mode': 'none', 'tokens_per_sample': 128, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_source_positions': None, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 2410, 'batch_size': 128, 'batch_size_valid': 128, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'moe_cross_entropy', 'moe_gate_loss_wt': 0.01, 'moe_gate_loss_combine_method': 'sum', 'moe_gate_loss_transform': 'none', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0007], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 1000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 5000.0, 'lr': [0.0007]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2023-11-10 02:33:37 | INFO | fairseq.tasks.language_modeling | dictionary: 33280 types
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:3 to store for rank: 0
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 2 nodes.
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:4 to store for rank: 0
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 2 nodes.
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:5 to store for rank: 0
2023-11-10 02:33:38 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 2 nodes.
2023-11-10 02:33:38 | INFO | fairseq_cli.train | LanguageModel(
  (decoder): LMDecoder(
    (dropout_module): Dropout(p=0.1, inplace=False)
    (embed_tokens): Embedding(33280, 256, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (output_projection): Linear(in_features=256, out_features=33280, bias=False)
    (layers): ModuleList(
      (0-3): 4 x BrainformerDecoderLayer(
        (dropout_module): Dropout(p=0.1, inplace=False)
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
          (dropout_module): Dropout(p=0.1, inplace=False)
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
        )
        (ffn1): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
        )
        (ffn2): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
        )
        (ffn3): FeedForwardNetwork(
          (activation_dropout_module): Dropout(p=0.0, inplace=False)
          (dropout_module): Dropout(p=0.1, inplace=False)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
        )
        (moe_layer): MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=256, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0-7): 8 x FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=256, out_features=512, bias=True)
              (fc2): Linear(in_features=512, out_features=256, bias=True)
            )
          )
        )
        (moe_layer1): MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=256, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0-7): 8 x FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=256, out_features=512, bias=True)
              (fc2): Linear(in_features=512, out_features=256, bias=True)
            )
          )
        )
        (moe_layer2): MOELayer(
          (gate): Top2Gate(
            (wg_reduction): Linear(in_features=256, out_features=16, bias=False)
          )
          (experts): ModuleList(
            (0-7): 8 x FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.1, inplace=False)
              (fc1): Linear(in_features=256, out_features=512, bias=True)
              (fc2): Linear(in_features=512, out_features=256, bias=True)
            )
          )
        )
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)
2023-11-10 02:33:38 | INFO | fairseq_cli.train | task: LanguageModelingTask
2023-11-10 02:33:38 | INFO | fairseq_cli.train | model: LanguageModel
2023-11-10 02:33:38 | INFO | fairseq_cli.train | criterion: MoECrossEntropyCriterion
2023-11-10 02:33:38 | INFO | fairseq_cli.train | num. non-expert model params: 13,848,064 (num. trained: 13,848,064)
2023-11-10 02:33:38 | INFO | fairseq_cli.train | num. expert model params: 25,239,552 (num. trained: 25,239,552)
2023-11-10 02:33:38 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0283203125, 'gpu_1_mem_used_gb': 1.0283203125}
2023-11-10 02:33:38 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: /home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/preprocess_scripts/data-bin/wikitext-2/valid
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.0.moe_layer.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.0.moe_layer1.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.0.moe_layer2.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer1.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.1.moe_layer2.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.2.moe_layer.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.2.moe_layer1.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.2.moe_layer2.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.3.moe_layer.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.3.moe_layer1.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- decoder.layers.3.moe_layer2.gate.wg_reduction.bias
2023-11-10 02:33:42 | INFO | fairseq.trainer | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0283203125, 'gpu_1_mem_used_gb': 1.0283203125}
^[[B^[[B^[[B^[[B^[[B^[[B^[[B2023-11-10 02:33:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2023-11-10 02:33:42 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.185 GB ; name = NVIDIA A100 80GB PCIe                   
2023-11-10 02:33:42 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 79.185 GB ; name = NVIDIA A100 80GB PCIe                   
2023-11-10 02:33:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2023-11-10 02:33:42 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2023-11-10 02:33:42 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 128
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B2023-11-10 02:33:42 | INFO | fairseq_cli.train | nvidia-smi stats: {'gpu_0_mem_used_gb': 1.0830078125, 'gpu_1_mem_used_gb': 1.0830078125}
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B2023-11-10 02:33:42 | INFO | fairseq.trainer | No existing checkpoint found ../checkpoints/xmoe_brainformer_lm_wt2_medium/checkpoint_last-rank-0.pt
2023-11-10 02:33:42 | INFO | fairseq.trainer | loading train data for epoch 1
2023-11-10 02:33:42 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: /home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/preprocess_scripts/data-bin/wikitext-2/train
epoch 001:   0%|                                                      | 0/4 [00:00<?, ?it/s]2023-11-10 02:33:42 | INFO | fairseq.trainer | begin training epoch 1
2023-11-10 02:33:42 | INFO | fairseq_cli.train | Start iterating over samples
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^CTraceback (most recent call last):
  File "/home/gtruong/.conda/envs/fairmoe/bin/fairseq-train", line 33, in <module>
                                                                                                sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/fairseq_cli/train.py", line 580, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/gtruong/Project/ICML/Fair_MOE/fairseq-moe/fairseq/distributed/utils.py", line 373, in call_main
    torch.multiprocessing.spawn(
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 109, in join
    ready = multiprocessing.connection.wait(
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
^CError in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
^CTraceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/multiprocessing/spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/site-packages/torch/__init__.py", line 1122, in <module>
    from .serialization import save, load
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/site-packages/torch/serialization.py", line 17, in <module>
    from typing_extensions import TypeAlias  # Python 3.10+
  File "/home/gtruong/.conda/envs/fairmoe/lib/python3.9/site-packages/typing_extensions.py", line 116, in <module>
    def _should_collect_from_parameters(t):
KeyboardInterrupt
/home/gtruong/.conda/envs/fairmoe/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

(fairmoe) bash-4.2$ exit
exit
